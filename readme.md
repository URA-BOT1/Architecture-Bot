# ğŸ™ï¸ Assistant Urbanisme - POC RAG + LLM

Ce projet est un **proof-of-concept (POC)** dâ€™assistant intelligent capable de rÃ©pondre Ã  des questions dâ€™urbanisme en sâ€™appuyant sur des documents locaux simulant les futures APIs. Il utilise un backend RAG + LLM et un frontend simple de type chat.

---

## ğŸ“¦ Contenu du projet

urbanisme-poc/
â”œâ”€â”€ backend/ # Code FastAPI + LLM
â”œâ”€â”€ frontend/ # Chat minimal en HTML/JS
â”œâ”€â”€ documents/ # DonnÃ©es locales simulant les APIs
â”œâ”€â”€ tests/ # (optionnel) Fichiers de tests
â”œâ”€â”€ .env.example # Variables dâ€™environnement
â”œâ”€â”€ .python-version # Version Python (pour Railway)
â”œâ”€â”€ Procfile # DÃ©marrage (pour Railway)
â”œâ”€â”€ requirements.txt # DÃ©pendances Python
â”œâ”€â”€ docker-compose.yml # (optionnel) Docker local
â”œâ”€â”€ Dockerfile # (optionnel)
â””â”€â”€ README.md # Ce fichier

yaml
Copier
Modifier

---

## ğŸš€ Fonctionnement

### ğŸ§  Pipeline

1. Utilisateur pose une question via lâ€™interface
2. FastAPI reÃ§oit la requÃªte
3. Le module `rag.py` cherche du contexte dans `/documents`
4. Le LLM (local) gÃ©nÃ¨re une rÃ©ponse Ã  partir du contexte
5. RÃ©ponse mise en cache Redis (via `cache.py`)
6. L'utilisateur reÃ§oit la rÃ©ponse dans le chat

---

## ğŸ› ï¸ Stack technique

| Ã‰lÃ©ment     | Outil utilisÃ©                     |
|-------------|-----------------------------------|
| **Backend** | FastAPI, LangChain, ChromaDB      |
| **LLM**     | Mistral 7B ou LLaMA 2 (local)     |
| **Embeddings** | sentence-transformers          |
| **Cache**   | Redis                             |
| **Frontend**| HTML + JS natif                   |
| **DÃ©ploiement** | Railway (Backend) + Vercel (Frontend) |

---

## ğŸ“ Backend â€“ fichiers clÃ©s

| Fichier           | RÃ´le                                     |
|-------------------|------------------------------------------|
| `main.py`         | EntrÃ©e FastAPI, route `/ask`             |
| `rag.py`          | Recherche contextuelle dans les docs     |
| `llm.py`          | Appel au modÃ¨le LLM local                |
| `cache.py`        | Cache Redis pour les requÃªtes            |
| `api_mock.py`     | Simule les futures APIs SOGEFI           |

---

## ğŸŒ Frontend â€“ `index.html`

Interface simple et statique (chat HTML + JS). Aucun changement requis tant que l'API backend respecte le format :

```json
{ "response": "Texte gÃ©nÃ©rÃ© par le LLM" }
ğŸ“‚ Documents locaux
Les sous-dossiers de documents/ contiennent les fichiers simulant les donnÃ©es dâ€™urbanisme :

plu/ â†’ Fichiers PDF

zonage/ â†’ DonnÃ©es gÃ©ographiques en JSON

reglements/ â†’ Textes bruts

ğŸ§ª Lancer en local
CrÃ©er un .env Ã  partir de .env.example

Installer les dÃ©pendances :

bash
Copier
Modifier
cd backend
pip install -r requirements.txt
Lancer FastAPI :

bash
Copier
Modifier
uvicorn main:app --reload
Ouvrir le fichier frontend/index.html dans un navigateur

â˜ï¸ DÃ©ploiement
Backend (Railway)
Repo connectÃ© Ã  Railway

Dossier root : /backend

NÃ©cessite :

.python-version

Procfile avec :

bash
Copier
Modifier
web: uvicorn main:app --host 0.0.0.0 --port $PORT
Frontend (Vercel)
Repo connectÃ© Ã  Vercel

Dossier root : /frontend

Pas de framework (juste HTML statique)

Lâ€™URL du backend doit Ãªtre dÃ©finie dans le JS :

js
Copier
Modifier
const API_URL = 'https://TON_BACKEND.up.railway.app';
âœ… Checklist de dÃ©ploiement rÃ©ussi
 API rÃ©pond correctement

 Frontend accessible depuis Vercel

 LLM local opÃ©rationnel

 Cache Redis fonctionnel

 Communication Front â†” Back assurÃ©e

ğŸ§  LeÃ§ons retenues
Ne pas mÃ©langer frontend/backend dans les configs

Utiliser .python-version pour Railway (et non runtime.txt)

Bien sÃ©parer la logique (RAG, LLM, API, cache)

Commencer simple cÃ´tÃ© UI pour itÃ©rer plus vite

ğŸ“Œ Ã€ venir
Connexion aux vraies APIs SOGEFI

Ajout dâ€™un vrai frontend React ou Vue

Upload dynamique de documents

Authentification utilisateur

ğŸ“ Lien GitHub
ğŸ‘‰ https://github.com/Volgine/Railway-Versel-Deploiment-Cloud-BOT-ARCHI-URBA

ğŸ Status : POC Fonctionnel âœ…
DÃ©ploiement rÃ©ussi

Architecture scalable

PrÃªt Ã  Ãªtre enrichi ğŸš€

yaml
Copier
Modifier

---

Souhaite-tu une **version traduite en anglais**, une version plus **orientÃ©e doc technique** (avec plus de dÃ©tails sur les endpoints et les formats JSON), ou câ€™est parfait comme Ã§a ?






Vous avez ditÂ :
c vrai que tu peux pas faire le fichier md direct.... 


ChatGPT a ditÂ :
Tu as raison : je ne peux pas crÃ©er ou push un fichier .md directement dans ton projet depuis ici, mais je peux te donner le contenu prÃªt Ã  copier-coller dans ton Ã©diteur local ou sur GitHub.

âœ… Voici comment faire :
Ouvre ton projet local (ou lâ€™Ã©diteur sur GitHub)

Va Ã  la racine du projet urbanisme-poc

CrÃ©e un fichier nommÃ© README.md

Copie-colle le contenu que je tâ€™ai donnÃ© ci-dessus dans ce fichier

Sauvegarde et commit :

bash
Copier
Modifier
git add README.md
git commit -m "ğŸ“ Ajout README complet pour le POC Urbanisme"
git push
Et voilÃ , il sera visible directement sur la page GitHub de ton repo.

Si tu veux, je peux aussi te gÃ©nÃ©rer un README.md version brute tÃ©lÃ©chargeable ici-mÃªme, ou te donner une version optimisÃ©e pour Notion, tu prÃ©fÃ¨res ?







Vous avez ditÂ :
lol pu la merde ton read me ces pas claire du tout , tu met le git clone a la fin alors que tout commece par git clone tes trop nul


ChatGPT a ditÂ :
Tâ€™as raison sur ce point. ğŸ’¯ Le git clone doit Ãªtre tout en haut, clair et visible, surtout pour ceux qui arrivent pour tester direct. Pas besoin de tourner autour du pot.

Voici une version remaniÃ©e, claire et directe, avec le bon ordre, un ton pro mais fluide, et un dÃ©but propre avec git clone en premier :

markdown
Copier
Modifier
# ğŸ™ï¸ Urbanisme POC â€“ Assistant RAG + LLM

> Assistant intelligent pour rÃ©pondre Ã  des questions dâ€™urbanisme Ã  partir de documents locaux (PLU, zonage, rÃ¨glements), avec backend RAG + LLM et frontend simple.

---

## ğŸš€ DÃ©marrage rapide

```bash
git clone https://github.com/Volgine/Railway-Versel-Deploiment-Cloud-BOT-ARCHI-URBA.git
cd Railway-Versel-Deploiment-Cloud-BOT-ARCHI-URBA
â–¶ï¸ Lancer en local
bash
Copier
Modifier
cd backend
pip install -r requirements.txt
uvicorn main:app --reload
Ouvre ensuite frontend/index.html dans ton navigateur.

ğŸ“ Structure du projet
bash
Copier
Modifier
urbanisme-poc/
â”œâ”€â”€ backend/            # API FastAPI + LLM local
â”‚   â”œâ”€â”€ main.py         # EntrÃ©e API
â”‚   â”œâ”€â”€ rag.py          # RÃ©cupÃ©ration contextuelle
â”‚   â”œâ”€â”€ llm.py          # IntÃ©gration LLM
â”‚   â”œâ”€â”€ cache.py        # Redis (cache)
â”‚   â”œâ”€â”€ api_mock.py     # Simulation dâ€™API urbanisme
â”‚   â”œâ”€â”€ .env.example
â”‚   â”œâ”€â”€ .python-version
â”‚   â”œâ”€â”€ Procfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ index.html      # Chat minimal (HTML/JS)
â”‚
â”œâ”€â”€ documents/          # ğŸ“„ DonnÃ©es locales simulÃ©es
â”‚   â”œâ”€â”€ plu/
â”‚   â”œâ”€â”€ zonage/
â”‚   â””â”€â”€ reglements/
ğŸ§  Fonctionnement global
Utilisateur pose une question via le frontend

Backend interroge les documents avec RAG

Contexte injectÃ© dans un LLM (local)

RÃ©ponse gÃ©nÃ©rÃ©e + mise en cache (Redis)

RÃ©ponse affichÃ©e dans le chat

ğŸ”§ Stack technique
Composant	Tech utilisÃ©e
Backend	FastAPI + LangChain + Chroma
LLM local	Mistral 7B / LLaMA 2
Embeddings	sentence-transformers
Cache	Redis
Frontend	HTML + JS natif
DÃ©ploiement	Railway (backend), Vercel (front)

â˜ï¸ DÃ©ploiement Cloud
Railway (Backend)
Dossier racine : /backend

.python-version â†’ 3.11

Procfile â†’

bash
Copier
Modifier
web: uvicorn main:app --host 0.0.0.0 --port $PORT
Supprime runtime.txt sâ€™il existe

Vercel (Frontend)
Dossier racine : /frontend

index.html inchangÃ©

API URL Ã  dÃ©finir dans le JS :

js
Copier
Modifier
const API_URL = 'https://archibottest2-production.up.railway.app';
âœ… Checklist finale
 Clonage repo OK

 API FastAPI opÃ©rationnelle

 Frontend connectÃ© et fonctionnel

 RÃ©ponses gÃ©nÃ©rÃ©es via LLM local

 Cache Redis actif

 DÃ©ploiement Railway + Vercel validÃ©

ğŸ“ Lien GitHub
ğŸ”— https://github.com/Volgine/Railway-Versel-Deploiment-Cloud-BOT-ARCHI-URBA

ğŸ§ª Tests et debug
Utilise curl ou Postman pour tester POST /ask

Logs Redis activÃ©s dans cache.py

ChromaDB intÃ©grÃ© en mÃ©moire par dÃ©faut

ğŸ“Œ Prochaine Ã©tape
Connexion Ã  vraies APIs SOGEFI

Upload fichiers dynamiques

UI plus avancÃ©e (React ou autre)

yaml
Copier
Modifier

---

Tu veux que je te file Ã§a en `.md` tÃ©lÃ©chargeable ici ou tu veux une version avec les **badges GitHub** et plus "pro style" ?






